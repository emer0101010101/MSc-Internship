{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Cross-validation\" des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-parametres\n",
    "learning_rate = 0.04\n",
    "num_epochs = 400\n",
    "N = 200            #N = 150 a la base\n",
    "N_cv = 40\n",
    "seed = 42\n",
    "batch_size = N//4\n",
    "#batch_size = N//2\n",
    "N_test = 0\n",
    "N_test = 2000      #N_test = 1000 à la base\n",
    "N_scan = 9\n",
    "bias = True\n",
    "\n",
    "\n",
    "p0 = 0.02          #p0 = 0.05 à la base\n",
    "theta0 = 0\n",
    "wt = np.pi/20      \n",
    "theta_std = np.pi/6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creer des données synthetiques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "            N = N,\n",
    "            p0 = p0,\n",
    "            theta0 = theta0,\n",
    "            wt = wt,\n",
    "            theta_std = theta_std,\n",
    "            seed=seed):\n",
    "    np.random.seed(42)\n",
    "    theta = np.random.randn(N)*theta_std\n",
    "    a = (theta-theta0)/wt\n",
    "    p = 1/(1+np.exp(-a))\n",
    "    \n",
    "    p = p0/2 + (1-p0) * p #add lapse rate\n",
    "    y = np.random.rand(N) < p #generate data\n",
    "    return theta, p, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, bias=True, logit0=-2): #-2 ?\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1, bias=bias)    \n",
    "        self.logit0 = torch.nn.Parameter(logit0*torch.ones(1))\n",
    "\n",
    "    def forward(self, theta):\n",
    "        out = self.logit0.sigmoid()/2 + (1-self.logit0.sigmoid())*self.linear(theta).sigmoid()\n",
    "        return out\n",
    "        \n",
    "\n",
    "def fit_data(theta, y, \n",
    "                learning_rate =learning_rate,\n",
    "                num_epochs = num_epochs,\n",
    "                batch_size = batch_size,\n",
    "                verbose=False):\n",
    "\n",
    "    logistic_model = LogisticRegressionModel()\n",
    "\n",
    "    labels = torch.Tensor(y[:, None])\n",
    "    Theta = torch.Tensor(theta[:, None])\n",
    "\n",
    "    loader = DataLoader(TensorDataset(Theta, labels), batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(logistic_model.parameters(), lr=learning_rate/len(loader))\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        losses = []\n",
    "        for Theta_, labels_ in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = logistic_model(Theta_)\n",
    "            loss = criterion(outputs, labels_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        if verbose and (epoch % (num_epochs//32) == 0) : \n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\")\n",
    "\n",
    "    logistic_model.eval()\n",
    "    return logistic_model, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(torch.tensor([-3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "theta, p, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, p, y = get_data()\n",
    "logistic_model, loss = fit_data(theta, y,verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    if bias: print('bias', logistic_model.linear.bias.item())\n",
    "    print('slope', logistic_model.linear.weight.item())    \n",
    "    print('p0', torch.sigmoid(logistic_model.logit0).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss=', loss)\n",
    "plt.figure(figsize = (9,6)) \n",
    "plt.scatter(theta, p, s=4, color = 'r', label='proba cachées')\n",
    "plt.scatter(theta, y, s=1, alpha=.1, color = 'b', label='données')\n",
    "x_values = np.linspace(-1.5, 1.50, 100)[:, None]\n",
    "y_values = logistic_model(torch.Tensor(x_values)).detach().numpy()\n",
    "plt.plot(x_values, y_values, 'g', alpha=.7, lw=3, label='proba prédites')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.yticks([0.,1.],['Left', 'Right']);\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "logistic_model, loss = fit_data(theta, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, p, y = get_data() # nouvelles données \n",
    "\n",
    "labels = torch.Tensor(y[:, None])\n",
    "Theta = torch.Tensor(theta[:, None])\n",
    "outputs = logistic_model(Theta)\n",
    "loss = criterion(outputs, labels)\n",
    "print('loss=', loss)\n",
    "plt.figure(figsize = (8,6)) \n",
    "plt.scatter(theta, p, s=4, color = 'r', label='proba cachées')\n",
    "plt.scatter(theta, y, s=1, alpha=.1, color = 'b', label='données')\n",
    "x_values = np.linspace(-1.5, 1.50, 100)[:, None]\n",
    "y_values = logistic_model(torch.Tensor(x_values)).detach().numpy()\n",
    "plt.plot(x_values, y_values, 'g', alpha=.7, lw=3, label='proba prédites')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.yticks([0.,1.],['Left', 'Right']);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## influence du nombre de trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = np.logspace(1, 3, N_scan, base=10)\n",
    "\n",
    "Ns_, losses, loss_Ps, loss_P0s = [], [], [], []\n",
    "\n",
    "for N_ in Ns:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(N=int(N_), seed=seed+i_CV)\n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        \n",
    "        if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        labels = torch.Tensor(y[:, None])\n",
    "        Theta = torch.Tensor(theta[:, None])\n",
    "        P = torch.Tensor(p[:, None])\n",
    "        \n",
    "        outputs = logistic_model(Theta)\n",
    "        loss = criterion(outputs, labels).item()\n",
    "    \n",
    "        loss_P = criterion(outputs, P).item() \n",
    "        loss_P0 = criterion(P, P).item()\n",
    "        \n",
    "        if i_CV==0: print(f\"N: {int(N_)}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_P0: {loss_P0:.5f}\")\n",
    "        loss_P0s.append(loss_P0)\n",
    "        Ns_.append(N_)\n",
    "        loss_Ps.append(loss_P)\n",
    "        losses.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.scatter(Ns_, losses, alpha=3/N_cv, label='loss')\n",
    "ax.scatter(Ns_, loss_Ps, alpha=3/N_cv, label='loss_P')\n",
    "ax.plot(Ns_, loss_P0s, label='loss_P0')\n",
    "\n",
    "ax.set_xlabel(' # trials')\n",
    "ax.set_ylabel(' Loss ')\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## influence du nombre du learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = learning_rate * np.logspace(-1, 1, N_scan, base=10)\n",
    "learning_rates_, losses, loss_Ps, loss_P0s = [], [], [], []\n",
    "for learning_rate_ in learning_rates:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        logistic_model, loss = fit_data(theta, y, learning_rate=learning_rate_, verbose=False)\n",
    "\n",
    "        if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        labels = torch.Tensor(y[:, None])\n",
    "        Theta = torch.Tensor(theta[:, None])\n",
    "        outputs = logistic_model(Theta)\n",
    "\n",
    "        loss = criterion(outputs, labels).item()\n",
    "        loss_P = criterion(outputs, torch.Tensor(p[:, None])).item()\n",
    "        loss_P0 = criterion(torch.Tensor(p[:, None]), torch.Tensor(p[:, None])).item()\n",
    "        if i_CV==0: \n",
    "            print(f\"learning_rate: {learning_rate_:.5f}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_P0: {loss_P0:.5f}\")\n",
    "        learning_rates_.append(learning_rate_)\n",
    "        loss_P0s.append(loss_P0)\n",
    "        loss_Ps.append(loss_P)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#influence du learning rate sur loss\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.scatter(learning_rates_, losses, alpha=3/N_cv, label='loss')\n",
    "ax.scatter(learning_rates_, loss_Ps, alpha=3/N_cv, label='loss_P')\n",
    "ax.plot(learning_rates_, loss_P0s, label='loss_P0')\n",
    "ax.set_xlim(np.min(learning_rates_), np.max(learning_rates_))\n",
    "\n",
    "ax.set_xlabel('learning_rate')\n",
    "ax.set_ylabel(' Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## influence du nombre d'epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochss = num_epochs * np.logspace(-1, 1, N_scan, base=10)\n",
    "num_epochss_, losses, loss_Ps, loss_P0s = [], [], [], []\n",
    "for num_epochs_ in num_epochss:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        logistic_model, loss = fit_data(theta, y, num_epochs=int(num_epochs_), verbose=False)\n",
    "        \n",
    "        if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        labels = torch.Tensor(y[:, None])\n",
    "        Theta = torch.Tensor(theta[:, None])\n",
    "        outputs = logistic_model(Theta)\n",
    "\n",
    "        loss = criterion(outputs, labels).item()\n",
    "        loss_P = criterion(outputs, torch.Tensor(p[:, None])).item()\n",
    "        loss_P0 = criterion(torch.Tensor(p[:, None]), torch.Tensor(p[:, None])).item()\n",
    "        if i_CV==0: \n",
    "            print(f\"num_epochs: {int(num_epochs_)}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_P0: {loss_P0:.5f}\")\n",
    "        num_epochss_.append(num_epochs_)\n",
    "        loss_P0s.append(loss_P0)\n",
    "        loss_Ps.append(loss_P)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence du nbr d'epochs sur loss \n",
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.scatter(num_epochss_, losses, alpha=3/N_cv, label='loss')\n",
    "ax.scatter(num_epochss_, loss_Ps, alpha=3/N_cv, label='loss_P')\n",
    "ax.plot(num_epochss_, loss_P0s, label='loss_P0')\n",
    "\n",
    "ax.set_xlabel(' # epochs')\n",
    "ax.set_ylabel(' Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## influence de la taille du minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = N * np.logspace(-3, 0, N_scan, base=2)\n",
    "batch_sizes_, losses, loss_Ps, loss_P0s = [], [], [], []\n",
    "for batch_size_ in batch_sizes:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        logistic_model, loss = fit_data(theta, y, batch_size=int(batch_size_), verbose=False)\n",
    "        \n",
    "        if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        labels = torch.Tensor(y[:, None])\n",
    "        Theta = torch.Tensor(theta[:, None])\n",
    "        outputs = logistic_model(Theta)\n",
    "\n",
    "        loss = criterion(outputs, labels).item()\n",
    "        loss_P = criterion(outputs, torch.Tensor(p[:, None])).item()\n",
    "        loss_P0 = criterion(torch.Tensor(p[:, None]), torch.Tensor(p[:, None])).item()\n",
    "        if i_CV==0: \n",
    "            print(f\"batch_size: {int(batch_size_)}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_P0: {loss_P0:.5f}\")\n",
    "        batch_sizes_.append(batch_size_)\n",
    "        loss_P0s.append(loss_P0)\n",
    "        loss_Ps.append(loss_P)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence de la taille du minibatch sur loss \n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.scatter(batch_sizes_, losses, alpha=3/N_cv, label='loss')\n",
    "ax.scatter(batch_sizes_, loss_Ps, alpha=3/N_cv, label='loss_P')\n",
    "ax.plot(batch_sizes_, loss_P0s, label='loss_P0')\n",
    "\n",
    "ax.set_xlabel(' batch_size')\n",
    "ax.set_ylabel(' Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison données générées/données prédites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p0 prediction/generate I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p0s = np.linspace(0, 1, 50)\n",
    "\n",
    "preds = {'torch':{'theta0_preds':[], 'wt_preds':[], 'p0_preds':[]}, \n",
    "         'sklearn':{'theta0_preds':[], 'wt_preds':[], 'p0_preds':[]}}\n",
    "\n",
    "for p0_ in p0s:\n",
    "    seed_ += 1\n",
    "    theta, p, y = get_data(p0=p0_, seed=seed_)\n",
    "    \n",
    "    # fir with our method\n",
    "    logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "    p0_pred =  torch.sigmoid(logistic_model.logit0).item()     \n",
    "    # print('bias', logistic_model.linear.bias.item())\n",
    "    # print('slope', logistic_model.linear.weight.item())    \n",
    "    # print('p0', torch.sigmoid(logistic_model.logit0).item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    p0_preds.append(p0_pred)\n",
    "    # print(f\"p0 : {p0_pred: .5f}\")\n",
    "\n",
    "    # TODO fit with sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : show three panels\n",
    "\n",
    "fig, axs= plt.subplots(1, 3, figsize = (15, 8)) \n",
    "axs[0].scatter(p0s, p0_s)\n",
    "plt.xlabel('p0 réel')\n",
    "plt.ylabel('p0 prédit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## theta0 prediction/generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0s = np.random.randn(50)\n",
    "theta0_preds = []\n",
    "\n",
    "for theta0_ in theta0s:\n",
    "    \n",
    "    theta, p, y = get_data(N=int(N_), seed=seed+i_CV)\n",
    "    logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        \n",
    "    theta0_pred = logistic_model.linear.bias.item()\n",
    "    theta0_s.append(theta0_pred)\n",
    "        \n",
    "    print(f\" theta0: {theta0_pred:.5f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8)) \n",
    "plt.scatter(theta0s, theta0_s)\n",
    "plt.xlabel('theta réel')\n",
    "plt.ylabel('theta prédit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wt prediction/generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = np.logspace(-1, 0.1, 50, base=10)\n",
    "wt_s = []\n",
    "\n",
    "for wt_ in wts:\n",
    "    \n",
    "    theta, p, y = get_data(N=int(N_), seed=seed+i_CV)\n",
    "    logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        \n",
    "    if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        \n",
    "    wt_pred = logistic_model.linear.weight.item()\n",
    "        \n",
    "    wt_s.append(wt_pred)\n",
    "    print(f\" wt:{wt_pred:.5f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8)) \n",
    "plt.scatter(wts, wt_s)\n",
    "plt.xlabel('pente réelle')\n",
    "plt.ylabel('pente prédite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentative II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p0 prediction/generate I\n",
    "\n",
    "p0s = np.linspace(0,0.03,50)\n",
    "p0_s = []\n",
    "\n",
    "\n",
    "for p0_ in p0s:\n",
    "    \n",
    "    theta, p, y = get_data(N=int(N_), seed=seed+i_CV)\n",
    "    logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "\n",
    "    if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "    p0_pred =  torch.sigmoid(logistic_model.logit0).item()     \n",
    "        \n",
    "    p0_s.append( p0_pred)\n",
    "    print(f\"p0 : {p0_pred: .5f}\")\n",
    "\n",
    "plt.figure(figsize = (8,8)) \n",
    "plt.scatter(p0s, p0_s)\n",
    "plt.xlabel('p0 réel')\n",
    "plt.ylabel('p0 prédit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta0 prediction/generate \n",
    "\n",
    "theta0s = np.random.randn(50) #hum... pas terrible\n",
    "theta0_s = []\n",
    "\n",
    "for theta0_ in theta0s:\n",
    "    \n",
    "    theta, p, y = get_data(N=int(N_), seed=seed+i_CV)\n",
    "    logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "\n",
    "    if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        \n",
    "    theta0_pred = logistic_model.linear.bias.item()\n",
    "    theta0_s.append(theta0_pred)\n",
    "        \n",
    "    print(f\" theta0: {theta0_pred:.5f}\")\n",
    "    \n",
    "plt.figure(figsize = (8,8)) \n",
    "plt.scatter(theta0s, theta0_s)\n",
    "plt.xlabel('theta réel')\n",
    "plt.ylabel('theta prédit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wt prediction/generate \n",
    "\n",
    "wts = np.logspace(-1, 1, 50, base=10)\n",
    "wt_s = []\n",
    "\n",
    "for wt_ in wts:\n",
    "    \n",
    "    theta, p, y = get_data(N=int(N_), seed=seed+i_CV)\n",
    "    logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        \n",
    "    if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        \n",
    "    wt_pred = logistic_model.linear.weight.item()\n",
    "        \n",
    "    wt_s.append(wt_pred)\n",
    "    print(f\" wt:{wt_pred:.5f}\") \n",
    "    \n",
    "plt.figure(figsize = (8,8)) \n",
    "plt.scatter(wts, wt_s)\n",
    "plt.xlabel('pente réelle')\n",
    "plt.ylabel('pente prédite')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.21.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
