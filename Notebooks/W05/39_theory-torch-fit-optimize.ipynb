{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Cross-validation\" des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.04\n",
    "num_epochs = 400\n",
    "N = 100\n",
    "N_cv = 40\n",
    "seed = 42\n",
    "batch_size = N//2\n",
    "batch_size = N//4\n",
    "\n",
    "N_test = 0\n",
    "N_test = 1000\n",
    "N_scan = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creer des données synthetiques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "            N = N,\n",
    "            p0 = 0.05,\n",
    "            theta0 = 0,\n",
    "            wt = np.pi/20,\n",
    "            theta_std = np.pi/6,\n",
    "            seed=seed):\n",
    "    np.random.seed(42)\n",
    "    theta = np.random.randn(N)*theta_std\n",
    "    a = (theta-theta0)/wt\n",
    "    p = 1/(1+np.exp(-a))\n",
    "    \n",
    "    p = p0/2 + (1-p0) * p\n",
    "    y = np.random.rand(N) < p\n",
    "    return theta, p, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, bias=True):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1, bias=bias)    \n",
    "        self.logit0 = torch.nn.Linear(1, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        p0 = torch.sigmoid(self.logit0(torch.zeros(1)))\n",
    "        out = p0/2 + (1-p0)*torch.sigmoid(self.linear(x))\n",
    "        return out\n",
    "\n",
    "def fit_data(theta, y, \n",
    "                learning_rate =learning_rate,\n",
    "                num_epochs = num_epochs,\n",
    "                batch_size = batch_size,\n",
    "                verbose=False):\n",
    "\n",
    "    logistic_model = LogisticRegressionModel()\n",
    "\n",
    "    labels = torch.Tensor(y[:, None])\n",
    "    Theta = torch.Tensor(theta[:, None])\n",
    "\n",
    "    loader = DataLoader(TensorDataset(Theta, labels), batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(logistic_model.parameters(), lr=learning_rate/len(loader))\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        losses = []\n",
    "        for Theta_, labels_ in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = logistic_model(Theta_)\n",
    "            loss = criterion(outputs, labels_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        if verbose and (epoch % (num_epochs//32) == 0) : \n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\")\n",
    "\n",
    "    logistic_model.eval()\n",
    "    return logistic_model, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "theta, p, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, p, y = get_data()\n",
    "logistic_model, loss = fit_data(theta, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6)) \n",
    "plt.scatter(theta, p, s=4, color = 'r', label='proba cachées')\n",
    "plt.scatter(theta, y, s=1, alpha=.1, color = 'b', label='données')\n",
    "x_values = np.linspace(-1.5, 1.50, 100)[:, None]\n",
    "y_values = logistic_model(torch.Tensor(x_values)).detach().numpy()\n",
    "plt.plot(x_values, y_values, 'g', alpha=.7, lw=3, label='proba prédites')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.yticks([0.,1.],['Left', 'Right']);\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "logistic_model, loss = fit_data(theta, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, p, y = get_data() # nouvelles données \n",
    "\n",
    "labels = torch.Tensor(y[:, None])\n",
    "Theta = torch.Tensor(theta[:, None])\n",
    "outputs = logistic_model(Theta)\n",
    "loss = criterion(outputs, labels)\n",
    "print('loss=', loss)\n",
    "plt.figure(figsize = (8,6)) \n",
    "plt.scatter(theta, p, s=4, color = 'r', label='proba cachées')\n",
    "plt.scatter(theta, y, s=1, alpha=.1, color = 'b', label='données')\n",
    "x_values = np.linspace(-1.5, 1.50, 100)[:, None]\n",
    "y_values = logistic_model(torch.Tensor(x_values)).detach().numpy()\n",
    "plt.plot(x_values, y_values, 'g', alpha=.7, lw=3, label='proba prédites')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.yticks([0.,1.],['Left', 'Right']);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## influence du nombre de trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = np.logspace(1, 3, N_scan, base=10)\n",
    "Ns_, losses, KLs = [], [], []\n",
    "for N_ in Ns:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(N=int(N_), seed=seed+i_CV)\n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        \n",
    "        if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        labels = torch.Tensor(y[:, None])\n",
    "        Theta = torch.Tensor(theta[:, None])\n",
    "        outputs = logistic_model(Theta)\n",
    "        loss = criterion(outputs, labels).item()\n",
    "        KL = criterion(outputs, torch.Tensor(p[:, None])).item()\n",
    "        if i_CV==0: print(f\"N: {int(N_)}, Loss: {loss:.5f}, KL: {KL:.5f}\")\n",
    "        Ns_.append(N_)\n",
    "        KLs.append(KL)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.scatter(Ns_, losses, alpha=3/N_cv, label='loss')\n",
    "ax.scatter(Ns_, KLs, alpha=3/N_cv, label='KL')\n",
    "\n",
    "ax.set_xlabel(' # trials')\n",
    "ax.set_ylabel(' Loss ')\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## influence du nombre du learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = learning_rate * np.logspace(-1, 1, N_scan, base=10)\n",
    "learning_rates_, losses, KLs = [], [], []\n",
    "for learning_rate_ in learning_rates:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        logistic_model, loss = fit_data(theta, y, learning_rate=learning_rate_, verbose=False)\n",
    "\n",
    "        if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        labels = torch.Tensor(y[:, None])\n",
    "        Theta = torch.Tensor(theta[:, None])\n",
    "        outputs = logistic_model(Theta)\n",
    "\n",
    "        loss = criterion(outputs, labels).item()\n",
    "        KL = criterion(outputs, torch.Tensor(p[:, None])).item()\n",
    "        if i_CV==0: \n",
    "            print(f\"learning_rate: {learning_rate_:.5f}, Loss: {loss:.5f}, KL: {KL:.5f}\")\n",
    "        learning_rates_.append(learning_rate_)\n",
    "        KLs.append(KL)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.scatter(learning_rates_, losses, alpha=3/N_cv, label='loss')\n",
    "ax.scatter(learning_rates_, KLs, alpha=3/N_cv, label='KL')\n",
    "\n",
    "ax.set_xlabel('learning_rate')\n",
    "ax.set_ylabel(' Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## influence du nombre d'epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochss = num_epochs * np.logspace(-1, 1, N_scan, base=10)\n",
    "num_epochss_, losses, KLs = [], [], []\n",
    "for num_epochs_ in num_epochss:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        logistic_model, loss = fit_data(theta, y, num_epochs=int(num_epochs_), verbose=False)\n",
    "        \n",
    "        if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        labels = torch.Tensor(y[:, None])\n",
    "        Theta = torch.Tensor(theta[:, None])\n",
    "        outputs = logistic_model(Theta)\n",
    "\n",
    "        loss = criterion(outputs, labels).item()\n",
    "        KL = criterion(outputs, torch.Tensor(p[:, None])).item()\n",
    "        if i_CV==0: \n",
    "            print(f\"num_epochs: {int(num_epochs_)}, Loss: {loss:.5f}, KL: {KL:.5f}\")\n",
    "        num_epochss_.append(num_epochs_)\n",
    "        KLs.append(KL)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.scatter(num_epochss_, losses, alpha=3/N_cv, label='loss')\n",
    "ax.scatter(num_epochss_, KLs, alpha=3/N_cv, label='KL')\n",
    "\n",
    "ax.set_xlabel(' # epochss')\n",
    "ax.set_xlim(learning_rates_.min(), learning_rates_.max())\n",
    "ax.set_ylabel(' Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## influence de la taille du minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = N * np.logspace(-3, 0, N_scan, base=2)\n",
    "batch_sizes_, losses, KLs = [], [], []\n",
    "for batch_size_ in batch_sizes:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        logistic_model, loss = fit_data(theta, y, batch_size=int(batch_size_), verbose=False)\n",
    "        \n",
    "        if N_test>0: theta, p, y = get_data(N=N_test) # nouvelles données \n",
    "        labels = torch.Tensor(y[:, None])\n",
    "        Theta = torch.Tensor(theta[:, None])\n",
    "        outputs = logistic_model(Theta)\n",
    "\n",
    "        loss = criterion(outputs, labels).item()\n",
    "        KL = criterion(outputs, torch.Tensor(p[:, None])).item()\n",
    "        if i_CV==0: \n",
    "            print(f\"batch_size: {int(batch_size_)}, Loss: {loss:.5f}, KL: {KL:.5f}\")\n",
    "        batch_sizes_.append(batch_size_)\n",
    "        KLs.append(KL)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.scatter(batch_sizes_, losses, alpha=3/N_cv, label='loss')\n",
    "ax.scatter(batch_sizes_, KLs, alpha=3/N_cv, label='KL')\n",
    "\n",
    "ax.set_xlabel(' batch_size')\n",
    "ax.set_ylabel(' Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.21.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
